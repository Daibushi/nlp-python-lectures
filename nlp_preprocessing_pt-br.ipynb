{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/logo_e-stude.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Introdução ao Processamento de Linguagem Natural (PLN) Usando Python </h1>\n",
    "<h3 align=\"center\"> Professor Fernando Vieira da Silva MSc.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Técnicas para Pré-Processamento </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Vamos avaliar as técnicas mais comuns para prepararmos o texto para usar com algoritmos de aprendizado de máquina logo mais.</p>\n",
    "<p>Como estudo de caso, vamos usar o texto de <i>Hamlet</i>, encontrado no corpus <i>Gutenberg</i> do pacote <b>NLTK</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Baixando o corpus Gutenberg</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/datascience/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Exibindo o texto \"Hamlet\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Hamlet by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo and Francisco two Centinels.\n",
      "\n",
      "  Barnardo. Who's there?\n",
      "  Fran. Nay answer me: Stand & vnfold\n",
      "your selfe\n",
      "\n",
      "   Bar. Long liue the King\n",
      "\n",
      "   Fran. Barnardo?\n",
      "  Bar. He\n",
      "\n",
      "   Fran. You come most carefully vpon your houre\n",
      "\n",
      "   Bar. 'Tis now strook twelue, get thee to bed Francisco\n",
      "\n",
      "   Fran. For this releefe much thankes: 'Tis bitter cold,\n",
      "And I am sicke at heart\n",
      "\n",
      "   Barn. Haue you had quiet Guard?\n",
      "  Fran. Not a Mouse stirring\n",
      "\n",
      "   Barn. Well, goodnight. If you do meet Horatio and\n",
      "Marcellus, the Riuals of my Watch, bid them make hast.\n",
      "Enter Horatio and Marcellus.\n",
      "\n",
      "  Fran. I thinke I heare them. Stand: who's there?\n",
      "  Hor. Friends to this ground\n",
      "\n",
      "   Mar. And Leige-men to the Dane\n",
      "\n",
      "   Fran. Giue you good night\n",
      "\n",
      "   Mar. O farwel honest Soldier, who hath relieu'd you?\n",
      "  Fra. Barnardo ha's my place: giue you goodnight.\n",
      "\n",
      "Exit Fran.\n",
      "\n",
      "  Mar. Holla Barnardo\n",
      "\n",
      "   Bar. Say, what is Horatio there?\n",
      "  Hor. A peece of\n"
     ]
    }
   ],
   "source": [
    "hamlet_raw = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\n",
    "print(hamlet_raw[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Segmentação de sentenças e tokenização de palavras</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[The Tragedie of Hamlet by William Shakespeare 1599]\\n\\n\\nActus Primus.', 'Scoena Prima.', 'Enter Barnardo and Francisco two Centinels.', 'Barnardo.', \"Who's there?\", 'Fran.', 'Nay answer me: Stand & vnfold\\nyour selfe\\n\\n   Bar.', 'Long liue the King\\n\\n   Fran.', 'Barnardo?', 'Bar.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(hamlet_raw)\n",
    "\n",
    "print(sentences[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']', 'Actus', 'Primus', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(sentences[0])\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4. Removendo stopwords e pontuação</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Tragedie', 'Hamlet', 'William', 'Shakespeare', '1599', ']', 'Actus', 'Primus', '.']\n"
     ]
    }
   ],
   "source": [
    "non_stopwords = [w for w in words if not w.lower() in stopwords_list]\n",
    "print(non_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuation = string.punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tragedie', 'Hamlet', 'William', 'Shakespeare', '1599', 'Actus', 'Primus']\n"
     ]
    }
   ],
   "source": [
    "non_punctuation = [w for w in non_stopwords if not w in punctuation]\n",
    "\n",
    "print(non_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5. Part of Speech (POS) Tags </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'IN'), ('The', 'DT'), ('Tragedie', 'NNP'), ('of', 'IN'), ('Hamlet', 'NNP'), ('by', 'IN'), ('William', 'NNP'), ('Shakespeare', 'NNP'), ('1599', 'CD'), (']', 'NNP'), ('Actus', 'NNP'), ('Primus', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As tags indicam a classificação sintática de cada palavra no texto. Ver <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\" target=\"blank\">https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a> para uma lista completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6. Stemming e Lemmatization</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming permite obter a \"raiz\" da palavra, removendo sufixos, por exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'has', 'alreadi', 'gone']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "sample_sentence = \"He has already gone\"\n",
    "sample_words = word_tokenize(sample_sentence)\n",
    "\n",
    "stems = [stemmer.stem(w) for w in sample_words]\n",
    "\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já lemmatization vai além de somente remover sufixos, obtendo a raiz linguística da palavra. Vamos usar as tags POS obtidas anteriormente para otimizar o lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'have', 'already', 'go']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "pos_tags = nltk.pos_tag(sample_words)\n",
    "\n",
    "lemmas = []\n",
    "for w in pos_tags:\n",
    "    if w[1].startswith('J'):\n",
    "        pos_tag = wordnet.ADJ\n",
    "    elif w[1].startswith('V'):\n",
    "        pos_tag = wordnet.VERB\n",
    "    elif w[1].startswith('N'):\n",
    "        pos_tag = wordnet.NOUN\n",
    "    elif w[1].startswith('R'):\n",
    "        pos_tag = wordnet.ADV\n",
    "    else:\n",
    "        pos_tag = wordnet.NOUN\n",
    "        \n",
    "    lemmas.append(lemmatizer.lemmatize(w[0], pos_tag))\n",
    "    \n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>7. N-gramas</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além da técnica de <i>Bag-of-Words</i>, outra opção é utilizar n-gramas (onde \"n\" pode variar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Tragedie of', 'Tragedie of Hamlet', 'of Hamlet by', 'Hamlet by William', 'by William Shakespeare', 'William Shakespeare 1599', 'Shakespeare 1599 Actus', '1599 Actus Primus']\n"
     ]
    }
   ],
   "source": [
    "non_punctuation = [w for w in words if not w.lower() in punctuation]\n",
    "\n",
    "n_grams_3 = [\"%s %s %s\"%(non_punctuation[i], non_punctuation[i+1], non_punctuation[i+2]) for i in range(0, len(non_punctuation)-2)]\n",
    "\n",
    "print(n_grams_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos usar a classe <b>CountVectorizer</b>, do scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[The Tragedie of Hamlet by William Shakespeare 1599]\\n\\n\\nActus Primus.']\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 0)\t1\n",
      "{'the tragedie of': 5, 'hamlet by william': 2, 'william shakespeare 1599': 7, 'of hamlet by': 3, 'tragedie of hamlet': 6, 'by william shakespeare': 1, '1599 actus primus': 0, 'shakespeare 1599 actus': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(3,3))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([sentences[0]])\n",
    "\n",
    "print(arr)\n",
    "\n",
    "n_gram_counts = count_vect.fit_transform(arr)\n",
    "\n",
    "print(n_gram_counts)\n",
    "\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos contar os n-grams (no nosso caso, trigramas) de todas as sentenças do texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 18380)\t1\n",
      "  (0, 20473)\t1\n",
      "  (0, 13004)\t1\n",
      "  (0, 6525)\t1\n",
      "  (0, 2993)\t1\n",
      "  (0, 22196)\t1\n",
      "  (0, 15662)\t1\n",
      "  (0, 0)\t1\n",
      "  (2, 4728)\t1\n",
      "  (2, 1884)\t1\n",
      "  (2, 661)\t1\n",
      "  (2, 5712)\t1\n",
      "  (6, 12125)\t1\n",
      "  (6, 1269)\t1\n",
      "  (6, 11049)\t1\n",
      "  (6, 16697)\t1\n",
      "  (6, 20823)\t1\n",
      "  (6, 23416)\t1\n",
      "  (7, 10278)\t1\n",
      "  (7, 10238)\t1\n",
      "  (7, 17966)\t1\n",
      "  (11, 22884)\t1\n",
      "  (11, 3373)\t1\n",
      "  (11, 11461)\t1\n",
      "  (11, 3125)\t1\n",
      "  :\t:\n",
      "  (13, 474)\t1\n",
      "  (13, 376)\t1\n",
      "  (13, 15974)\t1\n",
      "  (13, 1728)\t1\n",
      "  (14, 6885)\t1\n",
      "  (14, 22949)\t1\n",
      "  (14, 6494)\t1\n",
      "  (16, 12610)\t1\n",
      "  (16, 11592)\t1\n",
      "  (18, 8261)\t1\n",
      "  (18, 22902)\t1\n",
      "  (18, 4207)\t1\n",
      "  (18, 11162)\t1\n",
      "  (18, 7983)\t1\n",
      "  (18, 819)\t1\n",
      "  (18, 10805)\t1\n",
      "  (18, 18247)\t1\n",
      "  (18, 15131)\t1\n",
      "  (18, 13169)\t1\n",
      "  (18, 12059)\t1\n",
      "  (18, 21211)\t1\n",
      "  (18, 2339)\t1\n",
      "  (18, 18603)\t1\n",
      "  (19, 7983)\t1\n",
      "  (19, 4738)\t1\n",
      "['vnweeded garden that', 'stronger then either', 'to th court', 'walke in death', 'leysure but wilt', 'this physicke but', 'her paint an', 'neyther hauing the', 'cruell onely to', 'fall cursing like', 'norwey ouercome with', 'rose of the', 'my honor lord', 'sonne as twere', 'killes my father', 'fat vs and', 'maiesties might by', 'winnowed opinions and', 'that you vouchsafe', 'lady whilst this']\n"
     ]
    }
   ],
   "source": [
    "arr = np.array(sentences)\n",
    "\n",
    "n_gram_counts = count_vect.fit_transform(arr)\n",
    "\n",
    "print(n_gram_counts[:20])\n",
    "\n",
    "print([k for k in count_vect.vocabulary_.keys()][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
